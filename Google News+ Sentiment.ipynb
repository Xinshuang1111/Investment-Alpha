{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News_scrape for Google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the packages we need \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the package to scrape the content of a news page \n",
    "from goose3 import Goose  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Get the links of news in Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(key_word, n):\n",
    "    k = []\n",
    "    for i in range(0,n,10): #10 stands for data till page 1, 20 stands for data till page 2\n",
    "        search = 'https://www.google.com.mx/search?q=key_word&tbm=nws&ei=ThN-XvHFOr-zytMPjN2GQA&start=0'\n",
    "        url=search.replace('key_word',key_word)## change the search words\n",
    "        browser = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7' ## shown as web users\n",
    "        headers = {'User-Agent':browser,}\n",
    "        new_link = re.sub('start=\\d+','start=%s'%i, url, re.S)#collect links from pages we want\n",
    "        page = requests.get(new_link)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        links = soup.findAll(\"a\")  ## try to get the links we want\n",
    "        l = []  ## the list to store our news links\n",
    "        for link in  soup.find_all(\"a\",href=re.compile(\"(?<=/url\\?q=)(htt.*://.*)\")):\n",
    "            l.append(re.split(\":(?=http)\",link[\"href\"].replace(\"/url?q=\",\"\"))[0]) ## to decode the link for each page\n",
    "        for i in l:\n",
    "            k.append(i[:i.index('&sa')])\n",
    "    return(set(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 To scrape the content of the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "result = search(\"Jones+%26+Jones\",50) #collect news from latest 2 pages for Jones&Jones\n",
    "count_news = len(result) #the number of list of news crawped\n",
    "print(count_news)\n",
    "g = Goose()\n",
    "all_context = \"\"\n",
    "for i in result:\n",
    "    try:\n",
    "        article = g.extract(url=i)\n",
    "        all_context = all_context +article.cleaned_text +\"\\r\\n\"\n",
    "    except:\n",
    "        all_context = all_contect+\"\"\n",
    "\n",
    "#print(all_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'negitive_word.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ffcb9f3b3614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnegitive_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"negitive_word.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnegitive_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegitive_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnegitive_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegitive_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'negitive_word.txt'"
     ]
    }
   ],
   "source": [
    "negitive_dic = open(r\"negitive_word.txt\",encoding =\"utf8\", errors=\"ignore\")\n",
    "negitive_dic = negitive_dic.read()\n",
    "negitive_word = negitive_dic.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posstive_dic = open(r\"posstive_word.txt\",encoding =\"utf8\", errors=\"ignore\")\n",
    "posstive_dic = posstive_dic.read()\n",
    "posstive_word = posstive_dic.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(sentence):\n",
    "    seg_list = sentence.split()\n",
    "    seg_result = []\n",
    "    for w in seg_list:\n",
    "        seg_result.append(w)\n",
    "    return seg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stopWord(seg_result):\n",
    "    stop_dic = open('stop_word.txt')\n",
    "    stopwords = stop_dic.readlines()\n",
    "    arr = []\n",
    "    for stopword in stopwords:\n",
    "        stopword = stopword.replace(\"\\n\", \"\")\n",
    "        arr.append(stopword)\n",
    "    new_sent = []\n",
    "    for word in seg_result:\n",
    "        if word not in arr:\n",
    "            new_sent.append(word)\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_score(sentences):\n",
    "    seg_result = segmentation(sentences)\n",
    "    seg_result = del_stopWord(seg_result)\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "\n",
    "    for word in seg_result:\n",
    "        if word in negitive_word:\n",
    "            negcount +=1\n",
    "        elif word in posstive_word:\n",
    "            poscount +=1\n",
    "    final_score = (poscount - negcount)/(poscount + negcount)\n",
    "\n",
    "    print('Positive Score:',poscount)\n",
    "    print('Negative Score:',negcount)\n",
    "    print('Final Score:' ,final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 426\n",
      "Negative Score: 208\n",
      "Final Score: 0.3438485804416404\n"
     ]
    }
   ],
   "source": [
    "sentences = all_context\n",
    "sentence_score(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
